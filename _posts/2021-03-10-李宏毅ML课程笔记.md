## NTU李毅宏Machine Learning课程笔记

by Yixiang Ren

### 前言

此前就看到有关李宏毅老师的这门热门ML课程的推送，他在课程中会使用宝可梦等形象进行有趣的解释。正好最近本人在着手开始DL方面的开发，可以借此机会简单复习一下ML的基础知识。



### Lecture#1 Intro to ML&DL

机器学习任务的分类：回归Regression & 分类Classification，*Structured Learning结构化学习(自主学习生成有结构的物件)*

Regression问题的求解过程：

1. 设计一个模型，以最简单的线性回归为例。最初的模型设计最考验domain knowledge，即对于问题的理解

   $$
   y=wx_1+b
   $$

2. 定义一个损失函数L(b, w)（平均绝对值误差MAE、均方误差MSE、Cross-Entropy等）

   $$
   MSE:L=\frac1N\sum e_n=\frac1N\sum (y-\hat{y})^2
   $$

3. 最优化，常用梯度下降更新参数：

   $$
   w^*,b^*=arg\min L, \eta\rm\ is\ learning\ rate
   $$

   $$
   w_{i+1}\leftarrow w_i-\eta\frac{\partial L}{\partial w}|_{w=w_i,b=b_i}
   $$

   $$
   b_{i+1}\leftarrow b_i-\eta\frac{\partial L}{\partial b}|_{w=w_i,b=b_i}
   $$

   梯度下降可能会产生局部最优解

   
线性模型过于简单，对于更复杂的模型重复上述3步骤。实际上，任意连续曲线可以由分段线性模型(Piecewise Linear Curve)逼近，其中任意一个分段线性曲线=一系列sigmoid函数+常量b

$$
{\rm Sigmoid Fuction:}\ \ y=c\frac{1}{1+e^{-(b+wx_1)}}=c\ {\rm sigmoid}(b+wx_1)
$$

1. 任意连续曲线函数可以这样构建（已经可以看出MLP的样子了）：

   $$
   y=b+\sum_i c_i\ {\rm sigmoid}(b_i+w_ix_)=b+\sum_i c_i\ {\rm sigmoid}(b_i+\sum_jw_{ij}x_j)
   $$

2. 定义一个损失函数L(theta)，使用每一个batch的梯度张量更新theta参数：

   $$
   \rm{gradient}\ \boldsymbol g=\triangledown L(\boldsymbol{\theta}^0),\quad \boldsymbol{\theta}^1 \leftarrow \boldsymbol{\theta}^0-\eta \boldsymbol g
   $$
   ![截屏2021-03-11 下午2.01.09.png](https://github.com/Invisible-bob/Invisible-bob.github.io/blob/master/images/ML_slide/%E6%88%AA%E5%B1%8F2021-03-11%20%E4%B8%8B%E5%8D%882.01.09.png?raw=true)



         


### Lecture#2 Deep Learning

#### 2.1 ML Project Guidance

框架：1）建立一个未知方程   2）从训练数据定义Loss   3）令Loss最小迭代更新输出(Optimization)

若在ML项目实践中模型效果不好，先检查训练数据上的loss

- 训练数据上的loss很大
  - 模型可能过于简单，重新设计模型增加特征
  - Optimization步骤效果不好，可能梯度下降找到了局部最优解：可以尝试先训练较浅的模型进行对比
- 训练数据上的loss很小，测试集上的loss很大
  - 过拟合overfitting，解决方法如下
    - 增加训练数据丰富程度：增加数据量，数据增强Data Augmentation
    - 限制模型大小、深度与弹性，共用参数
    - Early Stopping
    - Regularization
    - Dropout
  - 训练数据和测试数据分布不同(mismatch)

合理的技巧：测试数据划分一小部分作为验证集Validation set，N-fold Cross Validation

#### 2.2 局部最小值 (local minima) 与鞍点 (saddle point)

梯度下降算法中，当梯度无限接近于0时loss就不会再减少：局部最小点和鞍点的梯度都为0 (critical point) 

*如何判断当前处于局部最小点还是鞍点？*：泰勒级数展开为2阶，梯度近似为0舍去1阶，通过计算Hessian矩阵的特征值符号判断是哪种点

$$
L(\theta)\approx L(\theta')+\frac12(\theta-\theta')^T\boldsymbol H(\theta-\theta'),\quad \boldsymbol H\rm\  is \ Hessian\ matrix
$$

**思考**：二维的local minima在三维可能就是个鞍点，在三维空间中存在梯度不为0的方向。同理local minima在更高维误差空间中可能就很少，大多数情况都存在梯度下降的路径





































